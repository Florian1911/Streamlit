import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import scipy as sc
import librosa
import soundfile as sf
import io
import libfmp.b
from scipy.signal import butter, filtfilt
from scipy.io.wavfile import write
from scipy.io import wavfile
from scipy.signal import fftconvolve
import tempfile
import os
from scipy.signal import lfilter

# Chemin absolu depuis le fichier Python courant
base_dir = os.path.dirname(__file__)
audio_path = os.path.join(base_dir, "assets", "stereo.wav")

# Fonction pour g√©n√©rer un chirp exponentiel (balayage fr√©quentiel progressif)
def generate_chirp_exp(dur, freq_start, freq_end, Fs=44100):
    N = int(dur * Fs)  # Nombre d'√©chantillons
    t = np.arange(N) / Fs  # Axe temporel
    freq = np.exp(np.linspace(np.log(freq_start), np.log(freq_end), N))  # √âvolution fr√©quentielle
    phases = np.zeros(N)
    for n in range(1, N):
        phases[n] = phases[n-1] + 2 * np.pi * freq[n-1] / Fs  # Phase cumul√©e
    x = np.sin(phases)  # Signal audio final
    return x, t, freq

def read_audio_from_wav(audio_file):
    audio_file.seek(0)
    signal, sample_rate = sf.read(audio_file)
    return signal, sample_rate

# Enregistrement du signal audio dans un fichier temporaire .wav
def save_audio_to_wav(signal, sample_rate, filename="output.wav"):
    """Sauvegarde le signal en fichier WAV sans le normaliser"""
    signal_int16 = np.int16(signal)  # Conversion en format 16 bits
    write(filename, sample_rate, signal_int16)
    return filename

st.title("Hear with my ears")

st.markdown("""
### üéØ Goal:
Compare how two people perceive the same sounds differently. This application allows one person to hear how the other person experiences sound.

### üß™ Steps:
- An **exponential chirp signal** (a sweep through frequencies) is generated to analyze the overall frequency spectrum.
- For each frequency, both individuals adjust the amplitude so the sound feels equally loud to them.
- The generated curves show the differences in perception between the two columns.
- A transfer function is calculated and then **applied to a real audio sample** to simulate how it would be perceived by the other person.

Take your time to listen to each frequency, adjust based on your perception, and observe how it affects the final sound output!
""")

# G√©n√©ration du signal Chirp exponentiel
st.header("Exponential Chirp and Analysis")

# Param√®tres de g√©n√©ration du chirp
freq_start = 30
freq_end = 18000
dur = 10
Fs = 44100  # Fr√©quence d'√©chantillonnage

x, t, freq = generate_chirp_exp(dur, freq_start=freq_start, freq_end=freq_end, Fs=Fs)

# Affichage du spectrogramme du chirp pour v√©rification visuelle
fig, ax = plt.subplots(figsize=(7, 3))
N, H = 1024, 512
X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N)  # Transform√©e de Fourier sur fen√™tres
libfmp.b.plot_matrix(np.log(1 + np.abs(X)), Fs=Fs / H, Fs_F=N / Fs, ax=[ax], title='Spectrogram of chirp', colorbar=False)
st.pyplot(fig)

# Lecture du son
st.audio(x, sample_rate=Fs)

# Test d'amplitude
st.header("Loudness Test - Amplitude Perception", help="Adjust the amplitude so that each frequency sounds equally loud to you.")

# Param√®tres de fr√©quence et dur√©e
FREQUENCIES = [125, 250, 500, 1000, 2000, 4000,6000, 8000,10000,12000,14000, 16000]
Fs = 44100  # Fr√©quence d'√©chantillonnage
DURATION = 2  # Dur√©e du son en secondes

amplitudes_selectionnees_col1 = []
amplitudes_selectionnees_col2 = []

# Liste des amplitudes possibles
amplitudes = [0.001, 0.01, 0.1, 0.5, 1, 2, 5]

# Liste pour stocker les amplitudes s√©lectionn√©es
amplitudes_selectionnees = []

MAX_INT16 = 32767 # Amplitude maximale pour un entier 16 bits
DURATION = 2

# Cr√©ation de deux colonnes pour les deux utilisateurs
col1, col2 = st.columns(2)

# Bloc pour la premi√®re colonne d‚Äôinteraction utilisateur
with col1:
    # Boucle sur chaque fr√©quence d√©finie dans FREQUENCIES
    for i, freq in enumerate(FREQUENCIES):
        # Affiche la fr√©quence courante
        st.write(f"Frequency : {freq} Hz")

        # Cr√©e un curseur pour permettre √† l‚Äôutilisateur de r√©gler l‚Äôamplitude du signal pour chaque fr√©quence
        amplitude_selectionnee = st.slider(
            f"Amplitude for {freq} Hz",  # L√©gende du slider
            min_value=0.0,               # Amplitude minimale (silence)
            max_value=1.0,               # Amplitude maximale (pleine √©chelle)
            value=0.5,                   # Valeur initiale par d√©faut
            step=0.01,                   # Pas du curseur
            key=f"col1_{i}",             # Cl√© unique pour √©viter les conflits dans Streamlit
            help="Select the signal amplitude for this frequency"  # Info bulle
        )
        # Enregistre l‚Äôamplitude s√©lectionn√©e dans une liste
        amplitudes_selectionnees_col1.append(amplitude_selectionnee)

        # G√©n√®re un vecteur de temps pour la dur√©e sp√©cifi√©e
        temps = np.linspace(0, DURATION, int(Fs * DURATION), endpoint=False)

        # G√©n√®re un signal sinuso√Ødal avec la fr√©quence et l‚Äôamplitude choisies
        signal = amplitude_selectionnee * MAX_INT16 * np.sin(2 * np.pi * freq * temps)

        # Convertit le signal en fichier WAV temporaire
        audio_file = save_audio_to_wav(signal, Fs)

        # Affiche un lecteur audio pour √©couter le signal g√©n√©r√©
        st.audio(audio_file, format="audio/wav")

    # Convertit les amplitudes en dB pour l'affichage graphique (√©vite les erreurs log(0))
    amplitudes_selectionnees_col1_db = 20 * np.log10(np.array(amplitudes_selectionnees_col1) + 1e-12)

    # V√©rifie si toutes les fr√©quences ont une amplitude s√©lectionn√©e
    if len(amplitudes_selectionnees_col1) == len(FREQUENCIES):
        # Affiche le titre du graphique
        st.write("Amplitude versus frequency curve (Column 1)")

        # Cr√©e le graphique Amplitude (en dB) vs Fr√©quence
        plt.figure(figsize=(8, 6))
        plt.plot(FREQUENCIES, amplitudes_selectionnees_col1_db, marker='o', linestyle='-', color='b')
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude (dB)")
        plt.title("Amplitude versus frequency curve (Column 1)")
        plt.grid(True)

        # Affiche le graphique dans Streamlit
        st.pyplot(plt)
    else:
        # Message si l'utilisateur n‚Äôa pas s√©lectionn√© toutes les amplitudes
        st.write("Please select an amplitude for each frequency in column 1.")


# Bloc pour la deuxi√®me colonne d‚Äôinteraction utilisateur
with col2:
    # Boucle sur chaque fr√©quence d√©finie dans FREQUENCIES
    for i, freq in enumerate(FREQUENCIES):
        # Affiche la fr√©quence courante
        st.write(f"Frequency : {freq} Hz")

        # Cr√©e un curseur pour permettre √† l‚Äôutilisateur de r√©gler l‚Äôamplitude du signal pour chaque fr√©quence
        amplitude_selectionnee = st.slider(
            f"Amplitude for {freq} Hz",  # L√©gende du slider
            min_value=0.0,               # Amplitude minimale
            max_value=1.0,               # Amplitude maximale
            value=0.5,                   # Valeur initiale
            step=0.01,                   # Pas d‚Äôajustement du slider
            key=f"col2_{i}",             # Cl√© unique pour √©viter les conflits avec les sliders de col1
            help="Select the signal amplitude for this frequency"  # Info bulle
        )
        # Enregistre l‚Äôamplitude s√©lectionn√©e dans une liste
        amplitudes_selectionnees_col2.append(amplitude_selectionnee)

        # G√©n√®re un vecteur temps pour la dur√©e sp√©cifi√©e
        temps = np.linspace(0, DURATION, int(Fs * DURATION), endpoint=False)

        # G√©n√®re le signal sinuso√Ødal avec la fr√©quence et l‚Äôamplitude s√©lectionn√©es
        signal = amplitude_selectionnee * MAX_INT16 * np.sin(2 * np.pi * freq * temps)

        # Sauvegarde le signal sous forme de fichier audio WAV
        audio_file = save_audio_to_wav(signal, Fs)

        # Affiche un lecteur audio dans Streamlit pour √©couter le signal
        st.audio(audio_file, format="audio/wav")

    # Convertit les amplitudes en dB pour le trac√© du graphique
    amplitudes_selectionnees_col2_db = 20 * np.log10(np.array(amplitudes_selectionnees_col2) + 1e-12)

    # V√©rifie que toutes les fr√©quences ont une amplitude s√©lectionn√©e
    if len(amplitudes_selectionnees_col2) == len(FREQUENCIES):
        # Affiche un titre descriptif du graphique
        st.write("Amplitude versus frequency curve (Column 2)")

        # Trace la courbe d‚Äôamplitude (en dB) en fonction de la fr√©quence
        plt.figure(figsize=(8, 6))
        plt.plot(FREQUENCIES, amplitudes_selectionnees_col2_db, marker='o', linestyle='-', color='r')
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude (dB)")
        plt.title("Amplitude versus frequency curve (Column 2)")
        plt.grid(True)

        # Affiche le graphique dans l‚Äôinterface Streamlit
        st.pyplot(plt)
    else:
        # Affiche un message si toutes les amplitudes ne sont pas encore r√©gl√©es
        st.write("Please select an amplitude for each frequency in column 2.")


# Bloc de traitement dans la premi√®re colonne (col1)
with col1:
    # V√©rifie que l'utilisateur a s√©lectionn√© une amplitude pour chaque fr√©quence dans les deux colonnes
    if len(amplitudes_selectionnees_col1) == len(FREQUENCIES) and len(amplitudes_selectionnees_col2) == len(FREQUENCIES):
        
        # Calcul de la fonction de transfert en dB (diff√©rence entre colonne 2 et colonne 1)
        transfer_function_db1 = amplitudes_selectionnees_col2_db - amplitudes_selectionnees_col1_db
        
        # Affichage de la courbe de diff√©rence d‚Äôamplitude
        st.header("Amplitude difference to be applied (2-1)")
        plt.figure(figsize=(8, 6))
        plt.plot(FREQUENCIES, transfer_function_db1, marker='o', linestyle='-', color='g')
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude Difference (dB)")
        plt.title("Amplitude difference to be applied (2-1)")
        plt.grid(True)
        st.pyplot(plt)

        # Titre pour la section de calcul IFFT (transformation inverse de Fourier)
        st.header("Inverse FFT")

        # √âtape 1 : Convertir les valeurs de dB vers une √©chelle lin√©aire
        transfer_function_linear1 = 10**(transfer_function_db1 / 20)

        # √âtape 2 : Cr√©er un spectre sym√©trique pour simuler une r√©ponse r√©elle
        # Partie positive du spectre (fr√©quences positives)
        positive_spectrum1 = transfer_function_linear1

        # Partie n√©gative du spectre : miroir de la partie positive (sym√©trie Hermitienne)
        negative_spectrum1 = positive_spectrum1[::-1]  # Inverse les √©l√©ments

        # On ajoute une amplitude "1" pour la fr√©quence 0 Hz dans la partie positive
        positive_spectrum1 = np.insert(positive_spectrum1, 0, 1)

        # Pr√©paration des fr√©quences pour l'affichage (axes X)
        negative_frequency1 = -np.array(FREQUENCIES[::-1])  # Fr√©quences n√©gatives (miroir)
        positive_frequency1 = np.array(FREQUENCIES)
        positive_frequency1 = np.insert(positive_frequency1, 0, 0)  # Ajoute 0 Hz

        # Fusion des fr√©quences pour former un spectre double (X-axis complet)
        DOUBLE_FREQUENCIES = np.concatenate((negative_frequency1, positive_frequency1))

        # Fusion des amplitudes pour former le spectre sym√©trique complet (Y-axis)
        full_spectrum_linear1 = np.concatenate((negative_spectrum1, positive_spectrum1))

        # Affiche le spectre sym√©trique complet en √©chelle lin√©aire
        st.subheader("Linear Symmetric Spectrum")
        plt.figure(figsize=(8, 6))
        plt.plot(DOUBLE_FREQUENCIES, full_spectrum_linear1)
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude")
        plt.title("Linear Symmetric Spectrum")
        plt.grid(True)
        st.pyplot(plt)

        # √âtape 4 : Appliquer la transformation de Fourier inverse (IFFT)
        time_domain_response1 = np.fft.ifft(full_spectrum_linear1).real  # Prend uniquement la partie r√©elle

        # Conversion de la r√©ponse temporelle en dB pour l‚Äôaffichage
        time_domain_response_db1 = 20 * np.log10(np.abs(time_domain_response1) + 1e-12)  # On ajoute 1e-12 pour √©viter log(0)

        # Affichage de la r√©ponse impulsionnelle estim√©e
        st.subheader("Estimated Impulse Response")
        plt.figure(figsize=(8, 6))
        plt.plot(time_domain_response_db1)
        plt.xlabel("Time")
        plt.ylabel("Amplitude (dB)")
        plt.title("Impulse Response")
        plt.grid(True)
        st.pyplot(plt)



# Bloc de traitement dans la deuxi√®me colonne (col2)
with col2:
    # V√©rifie que l‚Äôutilisateur a s√©lectionn√© une amplitude pour chaque fr√©quence dans les deux colonnes
    if len(amplitudes_selectionnees_col1) == len(FREQUENCIES) and len(amplitudes_selectionnees_col2) == len(FREQUENCIES):
        
        # Calcul de la fonction de transfert inverse : colonne 1 - colonne 2
        transfer_function_db2 = amplitudes_selectionnees_col1_db - amplitudes_selectionnees_col2_db
        
        # Affichage de la diff√©rence d'amplitude (fonction de transfert) dans le sens inverse
        st.header("Amplitude difference to be applied (1-2)")
        plt.figure(figsize=(8, 6))
        plt.plot(FREQUENCIES, transfer_function_db2, marker='o', linestyle='-', color='g')
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude Difference (dB)")
        plt.title("Amplitude difference to be applied (1-2)")
        plt.grid(True)
        st.pyplot(plt)

        # Titre pour la section de transformation de Fourier inverse
        st.header("Inverse FFT")

        # √âtape 1 : Convertir la fonction de transfert de dB √† une √©chelle lin√©aire
        transfer_function_linear2 = 10**(transfer_function_db2 / 20)

        # √âtape 2 : Cr√©er un spectre sym√©trique
        # Partie positive (fr√©quences mesur√©es)
        positive_spectrum2 = transfer_function_linear2

        # Partie n√©gative (miroir de la partie positive)
        negative_spectrum2 = positive_spectrum2[::-1]

        # Ajout de la composante continue (0 Hz) avec un gain unitaire
        positive_spectrum2 = np.insert(positive_spectrum2, 0, 1)

        # Pr√©paration des fr√©quences pour affichage (X-axis)
        negative_frequency2 = -np.array(FREQUENCIES[::-1])  # Fr√©quences n√©gatives
        positive_frequency2 = np.array(FREQUENCIES)
        positive_frequency2 = np.insert(positive_frequency2, 0, 0)  # Inclut 0 Hz
        DOUBLE_FREQUENCIES = np.concatenate((negative_frequency2, positive_frequency2))

        # Construction du spectre complet en amplitude
        full_spectrum_linear2 = np.concatenate((negative_spectrum2, positive_spectrum2))

        # Affichage du spectre lin√©aire sym√©trique
        st.subheader("Linear Symmetric Spectrum")
        plt.figure(figsize=(8, 6))
        plt.plot(DOUBLE_FREQUENCIES, full_spectrum_linear2)
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Amplitude")
        plt.title("Linear Symmetric Spectrum")
        plt.grid(True)
        st.pyplot(plt)

        # √âtape 4 : Calcul de la r√©ponse impulsionnelle via IFFT
        time_domain_response2 = np.fft.ifft(full_spectrum_linear2).real  # On garde uniquement la partie r√©elle

        # Conversion de la r√©ponse temporelle en dB (ajout d‚Äôun epsilon pour √©viter log(0))
        time_domain_response_db2 = 20 * np.log10(np.abs(time_domain_response2) + 1e-12)

        # Affichage de la r√©ponse impulsionnelle estim√©e
        st.subheader("Estimated Impulse Response")
        plt.figure(figsize=(8, 6))
        plt.plot(time_domain_response_db2)
        plt.xlabel("Time")
        plt.ylabel("Amplitude (dB)")
        plt.title("Impulse Response")
        plt.grid(True)
        st.pyplot(plt)

# -----------------------------------------
# Affichage du son original import√© par l'utilisateur
# -----------------------------------------

# Titre pour la section du son original
st.subheader("üîä Original sound")

# Lecture de l‚Äôaudio via Streamlit
st.audio(audio_path, format="audio/wav")

# Lecture des donn√©es audio avec scipy.io.wavfile
sample_rate, data = wavfile.read(audio_path)

# S√©paration des deux canaux st√©r√©o
left_channel = data[:, 0]  # Canal gauche
right_channel = data[:, 1]  # Canal droit


# Cr√©ation de deux colonnes c√¥te √† c√¥te
col3, col4 = st.columns(2)

# -----------------------------------
# Colonne 3 : Convolution et affichage du r√©sultat pour la colonne 1
# -----------------------------------
with col3:
    st.subheader("üîä Convolt result")

    # 1. Conversion des amplitudes en dB vers √©chelle lin√©aire
    amplitudes_lin = 10**(amplitudes_selectionnees_col1_db / 20)

    # 2. On ajoute une phase nulle (r√©elle, pas de partie imaginaire) pour la reconstruction du spectre
    spectre = amplitudes_lin

    # 3. Reconstruction du spectre complet (sym√©trie hermitienne) pour garantir un signal r√©el en temps
    # On prend le spectre et on y ajoute sa partie conjugu√©e invers√©e (sauf la premi√®re et derni√®re valeur)
    spectre_complet = np.concatenate([spectre, np.conj(spectre[-2:0:-1])])

    # 4. Calcul de la transform√©e de Fourier inverse (IFFT) pour obtenir le signal temporel
    signal_temps = np.fft.ifft(spectre_complet).real

    # 5. Convolution du signal original (gauche et droite) avec le signal temporel calcul√©
    convolved_left = fftconvolve(left_channel, signal_temps, mode="full")
    convolved_right = fftconvolve(right_channel, signal_temps, mode="full")

    # Recombinaison des canaux gauche et droite en un signal st√©r√©o
    convolved11 = np.stack([convolved_left, convolved_right], axis=1)

    # Affichage graphique du signal convolu√© en dB
    plt.figure(figsize=(8, 6))
    plt.plot(20 * np.log10(np.abs(convolved11)))
    plt.xlim(0, 10000)
    plt.xlabel("Time")
    plt.ylabel("Amplitude (dB)")
    plt.title("Convolved original sound with 1")
    plt.grid(True)
    st.pyplot(plt)

    # Normalisation du signal convolu√© pour √©viter la saturation audio
    convolved11 /= np.max(np.abs(convolved11) + 1e-6)

    # Sauvegarde en buffer m√©moire au format WAV
    buffer1 = io.BytesIO()
    sf.write(buffer1, convolved11, sample_rate, format='WAV')

    # Lecture audio du r√©sultat convolu√©
    st.audio(buffer1.getvalue(), format='audio/wav')

    st.subheader("üîä Convolved result")
    st.write("Audio perceived as 2")

    # Convolution avec la r√©ponse impulsionnelle estim√©e (time_domain_response1 calcul√©e pr√©c√©demment)
    convolved_left = fftconvolve(left_channel, time_domain_response1, mode="full")
    convolved_right = fftconvolve(right_channel, time_domain_response1, mode="full")

    # Recombinaison des canaux
    convolved1 = np.stack([convolved_left, convolved_right], axis=1)

    # Normalisation
    convolved1 /= np.max(np.abs(convolved1) + 1e-6)

    # Affichage graphique
    plt.figure(figsize=(8, 6))
    plt.plot(20 * np.log10(np.abs(convolved1)))
    plt.xlim(0, 10000)
    plt.xlabel("Time")
    plt.ylabel("Amplitude (dB)")
    plt.title("Convolved original sound with 1")
    plt.grid(True)
    st.pyplot(plt)

    # Sauvegarde et lecture audio
    buffer2 = io.BytesIO()
    sf.write(buffer2, convolved1, sample_rate, format='WAV')
    st.audio(buffer2.getvalue(), format='audio/wav')

# -----------------------------------
# Colonne 4 : Convolution et affichage du r√©sultat pour la colonne 2
# -----------------------------------
with col4:
    st.subheader("üîä Convolt result")

    # 1. Conversion dB ‚Üí lin√©aire pour la colonne 2
    amplitudes_lin = 10**(amplitudes_selectionnees_col2_db / 20)

    # 2. Phase nulle ajout√©e
    spectre = amplitudes_lin

    # 3. Reconstruction du spectre sym√©trique (Hermitien)
    spectre_complet = np.concatenate([spectre, np.conj(spectre[-2:0:-1])])

    # 4. Calcul IFFT ‚Üí signal temporel
    signal_temps = np.fft.ifft(spectre_complet).real

    # 5. Convolution avec le signal original st√©r√©o
    convolved_left = fftconvolve(left_channel, signal_temps, mode="full")
    convolved_right = fftconvolve(right_channel, signal_temps, mode="full")

    # Recombinaison st√©r√©o
    convolved22 = np.stack([convolved_left, convolved_right], axis=1)

    # Affichage graphique du signal convolu√©
    plt.figure(figsize=(8, 6))
    plt.plot(20 * np.log10(np.abs(convolved22)))
    plt.xlim(0, 10000)
    plt.xlabel("Time")
    plt.ylabel("Amplitude (dB)")
    plt.title("Convolved original sound with 2")
    plt.grid(True)
    st.pyplot(plt)

    # Normalisation pour √©viter saturation
    convolved22 /= np.max(np.abs(convolved22) + 1e-6)

    # Sauvegarde et lecture
    buffer1 = io.BytesIO()
    sf.write(buffer1, convolved22, sample_rate, format='WAV')
    st.audio(buffer1.getvalue(), format='audio/wav')

    st.subheader("üîä Convolved result")
    st.write("Audio perceived as 1")

    # Convolution avec la r√©ponse impulsionnelle estim√©e time_domain_response2
    convolved_left = fftconvolve(left_channel, time_domain_response2, mode="full")
    convolved_right = fftconvolve(right_channel, time_domain_response2, mode="full")

    # Recombinaison st√©r√©o
    convolved2 = np.stack([convolved_left, convolved_right], axis=1)

    # Affichage graphique du signal convolu√© avec la r√©ponse impulsionnelle
    plt.figure(figsize=(8, 6))
    plt.plot(20 * np.log10(np.abs(convolved2)))
    plt.xlim(0, 10000)
    plt.xlabel("Time")
    plt.ylabel("Amplitude (dB)")
    plt.title("Convolved original sound with 1")
    plt.grid(True)
    st.pyplot(plt)

    # Normalisation et sauvegarde finale
    convolved2 /= np.max(np.abs(convolved2) + 1e-6)

    buffer2 = io.BytesIO()
    sf.write(buffer2, convolved2, sample_rate, format='WAV')
    st.audio(buffer2.getvalue(), format='audio/wav')




with st.expander("üìò Theory explanations"):
    st.markdown("### üéß Why Fourier Transform?")
    st.markdown("""
To simulate how someone else hears, we analyze differences in sound perception **in the frequency domain** using the Discrete Fourier Transform (DFT):
""")
    st.latex(r"X[k] = \sum_{n=0}^{N-1} x[n] \cdot e^{-j 2\pi k n / N}")
    st.markdown(r"""
- \( x[n] \): audio signal in time domain  
- \( X[k] \): frequency components  
- \( N \): number of samples
""")

    st.markdown("---")
    st.markdown("### üîÑ Transfer Function")
    st.markdown("""
Once both users have adjusted how loud each frequency feels to them, we compute the difference in decibels:
""")
    st.latex(r"\Delta A_{\text{dB}}(f) = 20 \cdot \log_{10}\left(\frac{A_2(f)}{A_1(f)}\right)")

    st.markdown("""
This gives us the **difference in perception** for each frequency \( f \). We convert this to a linear scale:
""")
    st.latex(r"H(f) = 10^{\Delta A_{\text{dB}}(f)/20}")
    st.markdown("""
\( H(f) \) is the **transfer function**, a filter that transforms audio from one hearing profile to the other.
""")

    st.markdown("---")
    st.markdown("### ‚è™ Back to Time Domain")
    st.markdown("To apply this filter to a real sound, we convert \( H(f) \) back to the **time domain** using the inverse Fourier transform:")
    st.latex(r"h[n] = \text{IFFT}(H(f))")

    st.markdown("---")
    st.markdown("### üîä Final Step: Convolution")
    st.markdown("We apply the filter by convolving the real audio signal \( x[n] \) with the impulse response \( h[n] \):")
    st.latex(r"y[n] = x[n] * h[n] = \sum_{k=-\infty}^{\infty} x[k] \cdot h[n-k]")

    st.markdown("This results in a new sound ùë¶[ùëõ]y[n]: the original audio, transformed to simulate how the other person would perceive it.")




# import soundfile as sf
# import numpy as np

# # Charger le fichier mono
# data, samplerate = sf.read(audio_path)  # data: (N,)

# # V√©rifier que c'est bien mono
# if data.ndim == 1:
#     # Dupliquer le canal gauche pour cr√©er un signal st√©r√©o
#     stereo_data = np.stack([data, data], axis=1)  # shape: (N, 2)

#     # Sauvegarder en st√©r√©o
#     sf.write("stereo.wav", stereo_data, samplerate)
#     print("Fichier st√©r√©o cr√©√© avec succ√®s.")
# else:
#     print("Ce fichier est d√©j√† st√©r√©o.")
